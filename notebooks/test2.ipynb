{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## should make cfg.train.entity_embedding (=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in load_data.py\n",
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "    def __init__(self, dataset,labels,tokenizer,cfg):\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.new_tokens = []\n",
    "        self.marker_mode = cfg.train.marker_mode\n",
    "        if self.marker_mode == 'EMask':\n",
    "            self.new_tokens = ['<subj-ORG>','<subj-PER>','<obj-ORG>','<obj-PER>','<obj-DAT>','<obj-LOC>','<obj-POH>','<obj-NOH>']\n",
    "        elif self.marker_mode == \"EM\":\n",
    "            self.new_tokens = ['<subj>', '</subj>', '<obj>', '</obj>']\n",
    "        elif self.marker_mode == \"TEM\":\n",
    "            self.new_tokens = ['<s:ORG>', '<s:PER>', '<o:ORG>', '<o:PER>', '<o:DAT>', '<o:LOC>', '<o:POH>', '<o:NOH>', '</s:ORG>', '</s:PER>', '</o:ORG>', '</o:PER>', '</o:DAT>', '</o:LOC>', '</o:POH>', '</o:NOH>']\n",
    "        self.tokenizer.add_tokens(self.new_tokens)\n",
    "        \n",
    "        self.dataset = self.tokenizing(dataset)\n",
    "\n",
    "        self.cfg = cfg\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cfg.train.entity_embedding:\n",
    "            if len(self.labels) ==0:\n",
    "                return {'input_ids': torch.LongTensor(self.dataset[idx]['input_ids']).squeeze(0),\n",
    "                        'attention_mask': torch.LongTensor(self.dataset[idx]['attention_mask']).squeeze(0),\n",
    "                        'token_type_ids': torch.LongTensor(self.dataset[idx]['token_type_ids']).squeeze(0),\n",
    "                        'Entity_type_embedding': torch.LongTensor(self.dataset[idx]['Entity_type_embedding']).squeeze(0),\n",
    "                        'Entity_idxes': torch.LongTensor(self.dataset[idx]['Entity_idxes']).squeeze(0)                    \n",
    "                            }\n",
    "            else:\n",
    "                return {'input_ids': torch.LongTensor(self.dataset[idx]['input_ids']).squeeze(0),\n",
    "                        'attention_mask': torch.LongTensor(self.dataset[idx]['attention_mask']).squeeze(0),\n",
    "                        'token_type_ids': torch.LongTensor(self.dataset[idx]['token_type_ids']).squeeze(0),\n",
    "                        'Entity_type_embedding': torch.LongTensor(self.dataset[idx]['Entity_type_embedding']).squeeze(0),\n",
    "                        'Entity_idxes': torch.LongTensor(self.dataset[idx]['Entity_idxes']).squeeze(0),\n",
    "                        'labels' : torch.LongTensor([self.labels[idx]]).squeeze()}\n",
    "        else:\n",
    "            if len(self.labels) ==0:\n",
    "                return {'input_ids': torch.LongTensor(self.dataset[idx]['input_ids']).squeeze(0),\n",
    "                        'attention_mask': torch.LongTensor(self.dataset[idx]['attention_mask']).squeeze(0),\n",
    "                        'token_type_ids': torch.LongTensor(self.dataset[idx]['token_type_ids']).squeeze(0)                    \n",
    "                            }\n",
    "            else:\n",
    "                return {'input_ids': torch.LongTensor(self.dataset[idx]['input_ids']).squeeze(0),\n",
    "                        'attention_mask': torch.LongTensor(self.dataset[idx]['attention_mask']).squeeze(0),\n",
    "                        'token_type_ids': torch.LongTensor(self.dataset[idx]['token_type_ids']).squeeze(0),\n",
    "                        'labels' : torch.LongTensor([self.labels[idx]]).squeeze()}\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def tokenizing(self,dataframe):\n",
    "        data = []\n",
    "        for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "            # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "            text = self.add_special_enti(item,marker_mode = self.marker_mode)\n",
    "            # text = '[SEP]'.join([concat_entity, item['sentence']])\n",
    "            outputs = self.tokenizer(text, add_special_tokens=True,\n",
    "                                          truncation=True,\n",
    "                                          return_tensors=\"pt\",\n",
    "                                          padding='max_length',\n",
    "                                          max_length=256\n",
    "                                    )\n",
    "            data.append(outputs)\n",
    "        return data\n",
    "    \n",
    "    def add_special_enti(self,df,marker_mode= None):\n",
    "        def change_enti(sub,obj,marker_mode = None):\n",
    "            if marker_mode == 'TEM_punct':\n",
    "                Eng_type_to_Kor = {\"PER\":\"사람\", \"ORG\":\"단체\", \"POH\" : \"기타\", \"LOC\" : \"장소\", \"NOH\" : \"수량\", \"DAT\" : \"날짜\"}\n",
    "                marked_sub = ['@']+['*']+list(Eng_type_to_Kor[sub['type']]) + ['*']+list(sub['word'])+['@']\n",
    "                marked_obj = ['#']+['^']+list(Eng_type_to_Kor[obj['type']]) + ['^']+list(obj['word'])+['#']\n",
    "            elif marker_mode == 'TEM':\n",
    "                marked_sub = ['<s:']+list(sub['type']) + ['>']+list(sub['word'])+['</s:']+list(sub['type']) + ['>']\n",
    "                marked_obj = ['<o:']+list(obj['type']) + ['>']+list(obj['word'])+['</o:']+list(obj['type']) + ['>'] ## typo\n",
    "            elif marker_mode == \"EM\":\n",
    "                marked_sub = ['<subj>']+list(sub['word'])+['</subj>']\n",
    "                marked_obj = ['<obj>']+list(obj['word'])+['</obj>']\n",
    "            elif marker_mode == \"EMask\":\n",
    "                marked_sub = [f'<subj-{sub[\"type\"]}>']\n",
    "                marked_obj = [f'<obj-{obj[\"type\"]}>']\n",
    "            return marked_sub, marked_obj\n",
    "        marked = []\n",
    "        sub = eval(df['subject_entity'])\n",
    "        s_s, s_e = sub['start_idx'], sub['end_idx']+1\n",
    "        obj = eval(df['object_entity'])\n",
    "        o_s, o_e = obj['start_idx'], obj['end_idx']+1\n",
    "        marked_sub,marked_obj = change_enti(sub,obj,marker_mode = marker_mode)\n",
    "        if s_s < o_s:\n",
    "            marked += df['sentence'][:s_s]\n",
    "            marked += marked_sub\n",
    "            marked += df['sentence'][s_e:o_s]\n",
    "            marked += marked_obj\n",
    "            marked += df['sentence'][o_e:]\n",
    "            marked = ''.join(marked)\n",
    "        else:\n",
    "            marked += df['sentence'][:o_s]\n",
    "            marked += marked_obj\n",
    "            marked += df['sentence'][o_e:s_s]\n",
    "            marked += marked_sub\n",
    "            marked += df['sentence'][s_e:]\n",
    "            marked = ''.join(marked)\n",
    "        return marked\n",
    "\n",
    "    \n",
    "def load_data(dataset_dir):\n",
    "    \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    return pd_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in utils.py\n",
    "def get_entity_idxes(tokenizer, token_list, cfg):\n",
    "    \"\"\"\n",
    "        entity 표현 방식에 따른 entity 위치 계산\n",
    "    \"\"\"\n",
    "    entity_embedding = np.zeros(len(token_list))\n",
    "    if cfg.train.marker_mode == 'EM':\n",
    "        # 스페셜 토큰 위치로 쉽게 찾을 수 있음 ## [0,0,0,0,0,0,1,1,1,1,1,0,0,0,0] ## ['<subj>', '</subj>', '<obj>', '</obj>']\n",
    "        vocab_len = len(tokenizer)-4 ## special_token start_idx\n",
    "        subj_start_idx = np.where(token_list==vocab_len)[0][0]+1\n",
    "        subj_end_idx = np.where(token_list==vocab_len+1)[0][0]\n",
    "        obj_start_idx = np.where(token_list==vocab_len+2)[0][0]+1\n",
    "        obj_end_idx = np.where(token_list==vocab_len+3)[0][0]\n",
    "        entity_embedding[subj_start_idx:subj_end_idx] = 1\n",
    "        entity_embedding[obj_start_idx:obj_end_idx] = 2\n",
    "        \n",
    "        return entity_embedding, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx\n",
    "    elif cfg.train.marker_mode == 'EMask':\n",
    "        # entity word만 1로함. ## [0,0,0,1,1,1,1,0,0,0,2,2,2,0,0] ## ['<subj-ORG>','<subj-PER>','<obj-ORG>','<obj-PER>','<obj-DAT>','<obj-LOC>','<obj-POH>','<obj-NOH>']\n",
    "        subj_1 = tokenizer.convert_tokens_to_ids(['<subj-ORG>','<subj-PER>'])\n",
    "        obj_1 = tokenizer.convert_tokens_to_ids(['<obj-ORG>','<obj-PER>','<obj-DAT>','<obj-LOC>','<obj-POH>','<obj-NOH>'])\n",
    "\n",
    "        ## subj의 start_idx, end_idx를 찾는 과정. tokenized entity word 만 1로 구성할 것임.\n",
    "        ## '<subj-ORG>'  로 구성되어 있음.그래서 '<subj-ORG>'.idx만 찾아서 1로함\n",
    "        for idx, t in enumerate(token_list):\n",
    "            if (t in subj_1):\n",
    "                entity_embedding[idx] = 1\n",
    "                subj_start_idx = idx\n",
    "                subj_end_idx = idx+1\n",
    "                break\n",
    "\n",
    "        for idx, t in enumerate(token_list):\n",
    "            if (t in obj_1):\n",
    "                entity_embedding[idx] = 2\n",
    "                obj_start_idx = idx\n",
    "                obj_end_idx = idx+1\n",
    "                break\n",
    "\n",
    "        return entity_embedding, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx\n",
    "    elif cfg.train.marker_mode == 'TEM': ## check complete\n",
    "        # entity word만 1로함 ## [0,0,0,1,1,1,1,0,0,0,2,2,2,0,0] ## ['<s:ORG>', '<s:PER>', '<o:ORG>', '<o:PER>', '<o:DAT>', '<o:LOC>', '<o:POH>', '<o:NOH>', '</s:ORG>', '</s:PER>', '</o:ORG>', '</o:PER>', '</o:DAT>', '</o:LOC>', '</o:POH>', '</o:NOH>']\n",
    "        subj_1 = tokenizer.convert_tokens_to_ids(['<s:ORG>', '<s:PER>'])\n",
    "        subj_2 = tokenizer.convert_tokens_to_ids(['</s:ORG>', '</s:PER>'])\n",
    "        obj_1 = tokenizer.convert_tokens_to_ids(['<o:ORG>', '<o:PER>', '<o:DAT>', '<o:LOC>', '<o:POH>', '<o:NOH>'])\n",
    "        obj_2 = tokenizer.convert_tokens_to_ids(['</o:ORG>', '</o:PER>', '</o:DAT>', '</o:LOC>', '</o:POH>', '</o:NOH>'])\n",
    "\n",
    "        subj_start_idx = 0\n",
    "        subj_end_idx = 0\n",
    "        ## subj의 start_idx, end_idx를 찾는 과정. tokenized entity word 만 1로 구성할 것임.\n",
    "        ## '<s:ORG>' word '</s:ORG>'  로 구성되어 있음.그래서 '<s:ORG>'.idx + 1 = word의 첫 시작 token\n",
    "        for idx, t in enumerate(token_list):\n",
    "            if (t in subj_1):\n",
    "                subj_start_idx = idx + 1\n",
    "                subj_end_idx = subj_start_idx + 1\n",
    "                while token_list[subj_end_idx] not in subj_2:\n",
    "                    subj_end_idx += 1\n",
    "                break\n",
    "\n",
    "        entity_embedding[subj_start_idx:subj_end_idx] = 1\n",
    "\n",
    "        obj_start_idx = 0\n",
    "        obj_end_idx = 0\n",
    "        for idx, t in enumerate(token_list):\n",
    "            if (t in obj_1):\n",
    "                obj_start_idx = idx + 1\n",
    "                obj_end_idx = obj_start_idx + 1\n",
    "                while token_list[obj_end_idx] not in obj_2:\n",
    "                    obj_end_idx += 1\n",
    "                break\n",
    "\n",
    "        entity_embedding[obj_start_idx:obj_end_idx] = 2\n",
    "        return entity_embedding, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx\n",
    "    elif cfg.train.marker_mode == 'TEM_punct':\n",
    "    # 패턴을 이용해 찾기\n",
    "        subj_1 = tokenizer.convert_tokens_to_ids('@')\n",
    "        subj_2 = tokenizer.convert_tokens_to_ids('*')\n",
    "        obj_1 = tokenizer.convert_tokens_to_ids('#')\n",
    "        obj_2 = tokenizer.convert_tokens_to_ids('^')\n",
    "        names = tokenizer.convert_tokens_to_ids(['단체','사람','날짜','장소','기타','수량'])\n",
    "\n",
    "        subj_start_idx = 0\n",
    "        subj_end_idx = 0\n",
    "        ## subj의 start_idx, end_idx를 찾는 과정. tokenized entity word 만 1로 구성할 것임.\n",
    "        ## @ * type * word @ 로 구성되어 있음.그래서 @.idx + 4 = word의 첫 시작 token -> 이게 아닐 수도 있다. idx + 4 가 꼭 word의 시작점은 아님. type이 여러개의 token으로 tokenize될 수도 있음.\n",
    "        ## 한국어 PLM에 'ORG','DAT','LOC','POH','NOH'가 vocab에 없다. 물론 그대로 진행할 수도 있지만, TEM_punct의 성능 증가 전제에 맞지 않는다. 차라리 한국어로 번역해서 type을 넣어주는게 좋을 수도 있다.\n",
    "        for idx, t in enumerate(token_list):\n",
    "            if t == subj_1 and token_list[idx+1] == subj_2 and (token_list[idx+2] in names):\n",
    "                subj_start_idx = idx + 4\n",
    "                subj_end_idx = subj_start_idx + 1\n",
    "                while token_list[subj_end_idx] != subj_1:\n",
    "                    subj_end_idx += 1\n",
    "                break\n",
    "\n",
    "        entity_embedding[subj_start_idx:subj_end_idx] = 1\n",
    "\n",
    "        obj_start_idx = 0\n",
    "        obj_end_idx = 0\n",
    "        for idx, t in enumerate(token_list):\n",
    "            if t == obj_1 and token_list[idx+1] == obj_2 and (token_list[idx+2] in names):\n",
    "                obj_start_idx = idx + 4\n",
    "                obj_end_idx = obj_start_idx + 1\n",
    "                while token_list[obj_end_idx] != obj_1:\n",
    "                    obj_end_idx += 1\n",
    "                break\n",
    "        \n",
    "        entity_embedding[obj_start_idx:obj_end_idx] = 2\n",
    "        return entity_embedding, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx\n",
    "\n",
    "    return entity_embedding, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in utils.py\n",
    "def insert_entity_idx_tokenized_dataset(tokenizer, dataset, cfg):\n",
    "    \"\"\"\n",
    "    entity 표현 방식에 따른 entity 위치를 계산한 것 반환 받아 dataset에 넣어줍니다.\n",
    "    \"\"\"\n",
    "    for data in dataset:\n",
    "        entity_embeddings = []\n",
    "        entity_idxes = []\n",
    "        for ids in data['input_ids'].numpy():\n",
    "            entity_embedding, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx = get_entity_idxes(tokenizer, ids, cfg)\n",
    "            entity_embeddings.append(entity_embedding)\n",
    "            entity_idxes.append([subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx])\n",
    "        data['Entity_type_embedding'] = torch.tensor(entity_embeddings).to(torch.int64)\n",
    "        data['Entity_idxes'] = torch.tensor(entity_idxes).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizing: 100%|██████████| 25976/25976 [00:15<00:00, 1692.24it/s]\n",
      "tokenizing: 100%|██████████| 6494/6494 [00:03<00:00, 1687.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Start ==========\n",
      "========== END ==========\n"
     ]
    }
   ],
   "source": [
    "## in train_OmegaConf.py\n",
    "\n",
    "import pickle as pickle\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "import argparse\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer,EarlyStoppingCallback\n",
    "from utils import label_to_num\n",
    "\n",
    "cfg = OmegaConf.load('/opt/ml/baseline/code/config/config.yaml')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "## Model & Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.model_name)\n",
    "model_config = AutoConfig.from_pretrained(cfg.model.model_name)\n",
    "model_config.num_labels = 30\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.model.model_name, config=model_config)\n",
    "model.parameters\n",
    "model.to(device)\n",
    "\n",
    "## load dataset \n",
    "train_dataset = load_data(cfg.data.train_data)\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "\n",
    "# train_dev split, stratify 옵션으로 데이터 불균형 해결!\n",
    "train_data, dev_data, train_label, dev_label = train_test_split(train_dataset, train_label, test_size=0.2, random_state=cfg.train.seed, stratify=train_label)\n",
    "train_data.reset_index(drop=True, inplace = True)\n",
    "dev_data.reset_index(drop=True, inplace = True)\n",
    "\n",
    "## make dataset for pytorch\n",
    "RE_train_dataset = RE_Dataset(train_data, train_label, tokenizer, cfg)\n",
    "RE_dev_dataset = RE_Dataset(dev_data, dev_label, tokenizer, cfg)\n",
    "model.resize_token_embeddings(len(RE_train_dataset.tokenizer))\n",
    "\n",
    "if cfg.train.entity_embedding:\n",
    "    print('='*10, \"Start\", '='*10)\n",
    "    insert_entity_idx_tokenized_dataset(tokenizer, RE_train_dataset.dataset, cfg)\n",
    "    insert_entity_idx_tokenized_dataset(tokenizer, RE_dev_dataset.dataset, cfg)\n",
    "    print('='*10, \"END\", '='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.RE_Dataset object at 0x7fcc0a817c10>\n",
      "{'input_ids': tensor([    0,    36,    14,  3611,    14, 27048,  2132,    36,  4546,  2063,\n",
      "            7,    65,  3971,    65,  9555,    39,    10,    39,     7,   793,\n",
      "            6,    11, 10150,    11,  3621,  5116,  2125,  6100,  2377,  2391,\n",
      "         1283, 27135, 19227,   545,  1560,  2073,  3654,  2138,  3796,  2168,\n",
      "         2062,    16,  3744,  4084,  2170,  3618, 12483,  2069,  3663,  2318,\n",
      "          858,  2062,     6,  1072,     6,  3919,  2073,  4084,  2138,  4860,\n",
      "         2116,  2259,  4137,  2179,  3847,  5886, 21154,  3884,  2052,  1039,\n",
      "         2062,     6,   594,  1432,  2348,  3669,  2069, 11067,  2062,    18,\n",
      "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'Entity_type_embedding': tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'Entity_idxes': tensor([ 5,  7, 14, 18]), 'labels': tensor(6)}\n",
      "{'input_ids': tensor([[    0,    36,    14,  3611,    14, 27048,  2132,    36,  4546,  2063,\n",
      "             7,    65,  3971,    65,  9555,    39,    10,    39,     7,   793,\n",
      "             6,    11, 10150,    11,  3621,  5116,  2125,  6100,  2377,  2391,\n",
      "          1283, 27135, 19227,   545,  1560,  2073,  3654,  2138,  3796,  2168,\n",
      "          2062,    16,  3744,  4084,  2170,  3618, 12483,  2069,  3663,  2318,\n",
      "           858,  2062,     6,  1072,     6,  3919,  2073,  4084,  2138,  4860,\n",
      "          2116,  2259,  4137,  2179,  3847,  5886, 21154,  3884,  2052,  1039,\n",
      "          2062,     6,   594,  1432,  2348,  3669,  2069, 11067,  2062,    18,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'Entity_type_embedding': tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'Entity_idxes': tensor([[ 5,  7, 14, 18]])}\n",
      "tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(RE_train_dataset)\n",
    "print(RE_train_dataset[0])\n",
    "print(RE_train_dataset.dataset[0])\n",
    "print(RE_train_dataset.dataset[0]['Entity_type_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  7, 14, 18]])\n"
     ]
    }
   ],
   "source": [
    "print(RE_train_dataset.dataset[0]['Entity_idxes'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('level2_nlp_07')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "749b5bc89119f988f7a8a1115408a5b610211d422f1077e8fb6d448361fd692a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
