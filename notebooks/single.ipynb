{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d146bfd-72d5-4d64-a4eb-3c7e29c1d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe28604-e209-4653-8d55-03e5b78ca2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../dataset/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7400f30a-bdfb-45a5-9077-dfed37e3b4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                0\n",
       "sentence          〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...\n",
       "subject_entity    {'word': '비틀즈', 'start_idx': 24, 'end_idx': 26...\n",
       "object_entity     {'word': '조지 해리슨', 'start_idx': 13, 'end_idx':...\n",
       "label                                                   no_relation\n",
       "source                                                    wikipedia\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(df.iterrows())[1]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f6cb0ed-1ad6-4e33-a648-5ad98efda2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_enti(sub,obj,marker_mode = 'TEM_prunct'):\n",
    "    if marker_mode == 'TEM_prunct':\n",
    "        marked_sub = ['@']+['*']+list(sub['type']) + ['*']+list(sub['word'])+['@']\n",
    "        marked_obj = ['#']+['^']+list(obj['type']) + ['^']+list(obj['word'])+['#']\n",
    "    elif marker_mode == 'TEM':\n",
    "        marked_sub = ['<s:']+list(sub['type']) + ['>']+list(sub['word'])+['</s:']+list(sub['type']) + ['>']\n",
    "        marked_obj = ['<s:']+list(obj['type']) + ['>']+list(obj['word'])+['</s:']+list(obj['type']) + ['>']\n",
    "    elif marker_mode == \"EM\":\n",
    "        marked_sub = ['<subj>']+list(sub['word'])+['</subj']\n",
    "        marked_obj = ['<obj>']+list(obj['word'])+['</obj']\n",
    "    return marked_sub, marked_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "130ae4e2-d4fb-4430-89e3-c4f674d67303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_enti(df,marker_mode= 'TEM_prunct'):\n",
    "    def change_enti(sub,obj,marker_mode = 'TEM_prunct'):\n",
    "        if marker_mode == 'TEM_prunct':\n",
    "            marked_sub = ['@']+['*']+list(sub['type']) + ['*']+list(sub['word'])+['@']\n",
    "            marked_obj = ['#']+['^']+list(obj['type']) + ['^']+list(obj['word'])+['#']\n",
    "        elif marker_mode == 'TEM':\n",
    "            marked_sub = ['<s:']+list(sub['type']) + ['>']+list(sub['word'])+['</s:']+list(sub['type']) + ['>']\n",
    "            marked_obj = ['<s:']+list(obj['type']) + ['>']+list(obj['word'])+['</s:']+list(obj['type']) + ['>']\n",
    "        elif marker_mode == \"EM\":\n",
    "            marked_sub = ['<subj>']+list(sub['word'])+['</subj>']\n",
    "            marked_obj = ['<obj>']+list(obj['word'])+['</obj>']\n",
    "        return marked_sub, marked_obj\n",
    "    marked = []\n",
    "    sub = eval(df['subject_entity'])\n",
    "    s_s, s_e = sub['start_idx'], sub['end_idx']+1\n",
    "    obj = eval(df['object_entity'])\n",
    "    o_s, o_e = obj['start_idx'], obj['end_idx']+1\n",
    "    marked_sub,marked_obj = change_enti(sub,obj)\n",
    "    if s_s < o_s:\n",
    "        marked += df['sentence'][:s_s]\n",
    "        marked += marked_sub\n",
    "        marked += df['sentence'][s_e:o_s]\n",
    "        marked += marked_obj\n",
    "        marked += df['sentence'][o_e:]\n",
    "        marked = ''.join(marked)\n",
    "    else:\n",
    "        marked += df['sentence'][:o_s]\n",
    "        marked += marked_obj\n",
    "        marked += df['sentence'][o_e:s_s]\n",
    "        marked += marked_sub\n",
    "        marked += df['sentence'][s_e:]\n",
    "        marked = ''.join(marked)\n",
    "    return marked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f6f2072-aff8-4b68-9a75-a632a94847a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 32470/32470 [00:16<00:00, 1940.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenizing(dataframe):\n",
    "    data = []\n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "        # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "        text = add_special_enti(item)\n",
    "        # text = '[SEP]'.join([concat_entity, item['sentence']])\n",
    "        outputs = tokenizer(text, add_special_tokens=True,\n",
    "                                      padding=False\n",
    "                                )['input_ids']\n",
    "        data.append(outputs)\n",
    "    return data\n",
    "mx= tokenizing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39a0a187-e89c-4028-91fb-d001a24645ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 균일가 생활용품점 ( 주 ) @ * ORG * 아성다이소 @ ( 대표 # ^ PER ^ 박정부 # ) 는 코로나19 바이러스로 어려움을 겪고 있는 대구광역시에 행복박스를 전달했다고 10일 밝혔다. [SEP]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(mx[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43dd6d1c-fe15-4fa8-8d84-3e3fa7775271",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.Series(mx)\n",
    "# pd.DataFrame(ss,ss\n",
    "tmp = ss.apply(lambda x : len(x))\n",
    "sorted(tmp,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cbb559ae-9822-4848-8abd-4a390d804518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_entity    {'word': '서울대학교', 'start_idx': 283, 'end_idx':...\n",
      "object_entity     {'word': '1946년', 'start_idx': 89, 'end_idx': ...\n",
      "label                                                   no_relation\n",
      "Name: 13877, dtype: object\n",
      "1942년 경성치과전문학교 입학을 하였고 1943년 2월에서 1945년 2월까지 2년간 휴학 끝에 1945년 2월 경성치과전문학교 예과 2년에 복학 후 이듬해 1946년 경성치전(경성치과전문학교)이 서울대학교 치의과대학으로 개편되면서 서울대학교 치의과대학 본과에 편입을 하였는데 바로 그 해, 서울대학교 치과대학 본과 1년 재학 시절인 1946년 교내 연극반 반원으로 연극에 입문하였고 이후 한편으로 서울대학교 치과대학 본과 3년 시절이던 1948년 11월에 육군 군의무관 중위 임관하였고 1950년 2월 서울대학교 치의과대학 학사 학위하고 한 달 지난 1950년 3월에 육군 군의무관 대위 진급하였으며 이후 1950년 6월 25일에서 1951년 11월에 전역을 할 때까지 한국 전쟁에 육군 군의무관으로 참전했으며, 1951년 11월에 육군 군의무관 대위 예편하였다.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[13877][['subject_entity','object_entity','label']])\n",
    "print(df.iloc[13877]['sentence'])\n",
    "# 문장 긴거 좀 잘라주세요~~ ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bbef785-2166-447e-8dca-16f372a89084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "    def __init__(self, dataset,labels,tokenizer):\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.new_tokens = []\n",
    "        if self.args.input_format == 'entity_marker':\n",
    "            self.new_tokens = ['<subj>', '</subj>', '<obj>', '</obj>']\n",
    "        self.tokenizer.add_tokens(self.new_tokens)\n",
    "        self.dataset = self.tokenizing(dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.labels) ==0:\n",
    "            return {'input_ids': torch.LongTensor(self.dataset[idx]['input_ids']).squeeze(0),\n",
    "                    'attention_mask': torch.LongTensor(self.dataset[idx]['attention_mask']).squeeze(0),\n",
    "                    'token_type_ids': torch.LongTensor(self.dataset[idx]['token_type_ids']).squeeze(0)\n",
    "                           }\n",
    "        else:\n",
    "            return {'input_ids': torch.LongTensor(self.dataset[idx]['input_ids']).squeeze(0),\n",
    "                    'attention_mask': torch.LongTensor(self.dataset[idx]['attention_mask']).squeeze(0),\n",
    "                    'token_type_ids': torch.LongTensor(self.dataset[idx]['token_type_ids']).squeeze(0),\n",
    "                    'labels' : torch.LongTensor([self.labels[idx]]).squeeze()}\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def tokenizing(self,dataframe):\n",
    "        data = []\n",
    "        for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "            # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "            text = self.add_special_enti(item)\n",
    "            # text = '[SEP]'.join([concat_entity, item['sentence']])\n",
    "            outputs = self.tokenizer(text, add_special_tokens=True,\n",
    "                                          truncation=True,\n",
    "                                          return_tensors=\"pt\",\n",
    "                                          padding='max_length',\n",
    "                                          max_length=256\n",
    "                                    )\n",
    "            data.append(outputs)\n",
    "        return data\n",
    "    \n",
    "    def add_special_enti(self,df,marker_mode= 'TEM_prunct'):\n",
    "        def change_enti(sub,obj,marker_mode = 'TEM_prunct'):\n",
    "            if marker_mode == 'TEM_prunct':\n",
    "                marked_sub = ['@']+['*']+list(sub['type']) + ['*']+list(sub['word'])+['@']\n",
    "                marked_obj = ['#']+['^']+list(obj['type']) + ['^']+list(obj['word'])+['#']\n",
    "            elif marker_mode == 'TEM':\n",
    "                marked_sub = ['<s:']+list(sub['type']) + ['>']+list(sub['word'])+['</s:']+list(sub['type']) + ['>']\n",
    "                marked_obj = ['<s:']+list(obj['type']) + ['>']+list(obj['word'])+['</s:']+list(obj['type']) + ['>']\n",
    "            elif marker_mode == \"EM\":\n",
    "                marked_sub = ['<subj>']+list(sub['word'])+['</subj>']\n",
    "                marked_obj = ['<obj>']+list(obj['word'])+['</obj>']\n",
    "            return marked_sub, marked_obj\n",
    "        marked = []\n",
    "        sub = eval(df['subject_entity'])\n",
    "        s_s, s_e = sub['start_idx'], sub['end_idx']+1\n",
    "        obj = eval(df['object_entity'])\n",
    "        o_s, o_e = obj['start_idx'], obj['end_idx']+1\n",
    "        marked_sub,marked_obj = change_enti(sub,obj)\n",
    "        if s_s < o_s:\n",
    "            marked += df['sentence'][:s_s]\n",
    "            marked += marked_sub\n",
    "            marked += df['sentence'][s_e:o_s]\n",
    "            marked += marked_obj\n",
    "            marked += df['sentence'][o_e:]\n",
    "            marked = ''.join(marked)\n",
    "        else:\n",
    "            marked += df['sentence'][:o_s]\n",
    "            marked += marked_obj\n",
    "            marked += df['sentence'][o_e:s_s]\n",
    "            marked += marked_sub\n",
    "            marked += df['sentence'][s_e:]\n",
    "            marked = ''.join(marked)\n",
    "        return marked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad830772-abcf-4887-aa77-cb8dae04e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('../dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label\n",
    "label = label_to_num(df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7b4d720-9208-4d26-935c-72c2d0bf94dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 32470/32470 [00:21<00:00, 1493.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "dataset = RE_Dataset(df,label,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b272fe9-fb48-4366-bc03-0c3cbf48f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token_dict = {'additional_special_tokens' : ['<sub_PER>','</sub_PER>','<sub_ORG>','</sub_ORG>','<sub_LOC>','</sub_LOC>',₩\n",
    "                                                     '<sub_DAT>','</sub_DAT>','<sub_POH>','</sub_POH>','<sub_NOH>','</sub_NOH>'₩\n",
    "                                                     '<obj_PER>','</obj_PER>','<obj_ORG>','</obj_ORG>','<obj_LOC>','</obj_LOC>',₩\n",
    "                                                     '<obj_DAT>','</obj_DAT>','<obj_POH>','</obj_POH>','<obj_NOH>','</obj_NOH>']}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "164ac6c8-e2d1-4672-84d8-e989b657c13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,   168, 30985, 14451,  7088,  4586,   169,   793,    32, 21639,\n",
       "            34,  8373, 14113,  2234,    32,    19, 21639,    34,  1504,  1363,\n",
       "          2088,    32,    51,  2107,  2341,    34, 29830,    32,    19,    51,\n",
       "          2107,  2341,    34,   543, 14879,  2440,  6711,   170, 21406, 26713,\n",
       "          2076, 25145,  5749,   171,  1421,   818,  2073,  4388,  2062,    18,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46db86-a766-4218-bd9d-ccf2a84c8c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
